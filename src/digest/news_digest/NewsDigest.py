import json
import os
import random
import re
from xml.dom import minidom
from xml.etree.ElementTree import (Element, SubElement, register_namespace,
                                   tostring)

from openai import OpenAI
from utils import File, JSONFile, Log, Time, TimeFormat, TimeUnit

from digest.article import Article
from digest.news_digest.NewsDigestReadMeMixin import NewsDigestReadMeMixin

log = Log("NewsDigest")


class NewsDigest(NewsDigestReadMeMixin):
    MAX_CONTENT_LEN = 1_000_000
    MAX_DAYS_OLD = 7
    MODEL = "gpt-5"
    DIR_DIGESTS = os.path.join("data", "digests")
    URL = "https://github.com/nuuuwan/lk_news_digest/blob/main/README.md"
    RSS_FEED_URL = (
        "https://raw.githubusercontent.com"
        + "/nuuuwan/lk_news_digest/refs/heads/main/rss.xml"
    )

    @staticmethod
    def __get_article_in_time_window__() -> list[Article]:
        articles = Article.list_all()
        min_time_ut = (
            Time.now().ut - NewsDigest.MAX_DAYS_OLD * TimeUnit.SECONDS_IN.DAY
        )
        articles_in_time_window = [
            a for a in articles if a.time_ut >= min_time_ut
        ]
        log.debug(
            f"Found {len(articles_in_time_window)} articles in time window."
        )
        return articles_in_time_window

    @staticmethod
    def __get_news_article_content__() -> str:
        articles_in_time_window = NewsDigest.__get_article_in_time_window__()

        content = ""
        used_articles = []
        total_len = 0
        random.shuffle(articles_in_time_window)
        for article in articles_in_time_window:
            total_len += len(article.all_text) + 3
            used_articles.append(article)
            if total_len > NewsDigest.MAX_CONTENT_LEN:
                break
        content = "\n...\n".join([a.all_text for a in used_articles])
        used_articles.sort(key=lambda a: a.time_ut, reverse=True)
        log.debug(
            f"Selected {len(content):,}B content"
            + f" based on {len(used_articles)} articles random articles"
            + f" from {len(articles_in_time_window)}."
        )
        return content, used_articles

    @staticmethod
    def __get_system_prompt__() -> str:
        return File(os.path.join("prompts", "digest.json.txt")).read().strip()

    @staticmethod
    def __validate_digest_articles__(digest_article_list):
        n_digest_articles = len(digest_article_list)
        assert n_digest_articles > 0, "No items in digest data"
        first_item = digest_article_list[0]
        assert "title" in first_item, "No title in first item"
        assert "body" in first_item, "No body in first item"

    @staticmethod
    def __get_digest_article_list__(
        system_prompt, news_article_content, ts
    ) -> list[dict]:

        return JSONFile(
            os.path.join("data", "digests", "digest.20251113.110349.json")
        ).read()

        log.debug(f"Generating digest with MODEL={NewsDigest.MODEL}...")
        client = OpenAI()
        response = client.responses.create(
            model=NewsDigest.MODEL,
            reasoning={"effort": "low"},
            input=[
                {
                    "role": "system",
                    "content": system_prompt,
                },
                {
                    "role": "user",
                    "content": news_article_content,
                },
            ],
        )
        digest_article_list = json.loads(response.output_text)
        NewsDigest.__validate_digest_articles__(digest_article_list)
        log.info(f"Generated digest with {len(digest_article_list)} items.")

        os.makedirs(NewsDigest.DIR_DIGESTS, exist_ok=True)
        digest_path = os.path.join(
            NewsDigest.DIR_DIGESTS, f"digest.{ts}.json"
        )
        digest_file = JSONFile(digest_path)
        digest_file.write(digest_article_list)
        log.info(f"Wrote {digest_file}")

        return digest_article_list

    @staticmethod
    def get_title() -> str:
        return "# ðŸ‡±ðŸ‡° Sri Lanka This Week"

    @staticmethod
    def get_description(used_articles) -> str:
        date_strs = [a.date_str for a in used_articles]
        min_date_str = min(date_strs)
        max_date_str = max(date_strs)
        n = len(used_articles)

        return (
            f"*Generated by [{NewsDigest.MODEL}]({NewsDigest.MODEL_URL})"
            + f" from **{n:,}** English News Articles published"
            + f" between **{min_date_str}** & **{max_date_str}**.*"
        )

    DIGEST_RSS_PATH = "rss.xml"

    @staticmethod
    def get_deep_link_for_title(title: str) -> str:
        # remove all non alphanumeric characters except spaces
        title = re.sub(r"[^a-zA-Z0-9 ]", "", title)
        title = title.replace(" ", "-").lower()
        return title

    def build_rss_xml_data(self, used_articles, digest_article_list, ut, ts):

        register_namespace("atom", "http://www.w3.org/2005/Atom")
        rss = Element(
            "rss",
            {"version": "2.0"},
        )
        channel = SubElement(rss, "channel")
        SubElement(
            channel,
            "{http://www.w3.org/2005/Atom}link",
            {
                "href": self.RSS_FEED_URL,
                "rel": "self",
                "type": "application/rss+xml",
            },
        )

        pub_time_str = TimeFormat("%a, %d %b %Y %H:%M:%S %z").format(Time(ut))

        for tag, text in [
            ("title", self.get_title()),
            ("link", self.URL),
            ("description", self.get_description(used_articles)),
            (
                "lastBuildDate",
                pub_time_str,
            ),
        ]:
            SubElement(channel, tag).text = text

        history_url = self.get_history_url(ts)
        for i_article, digest_article in enumerate(
            digest_article_list, start=1
        ):
            item = SubElement(channel, "item")
            SubElement(item, "title").text = digest_article["title"]
            SubElement(item, "description").text = digest_article["body"]
            SubElement(item, "pubDate").text = pub_time_str
            deep_link_title = self.get_deep_link_for_title(
                digest_article["title"]
            )
            guid = f"{history_url}#{i_article}-{deep_link_title}"
            SubElement(item, "guid").text = guid

        rough = tostring(rss, encoding="utf-8", xml_declaration=True)
        parsed = minidom.parseString(rough)
        return parsed.toprettyxml(indent="  ", encoding="utf-8")

    def build_rss(self, used_articles, digest_article_list, ut, ts):
        rss_xml_data = self.build_rss_xml_data(
            used_articles, digest_article_list, ut, ts
        )
        with open(self.DIGEST_RSS_PATH, "wb") as f:
            f.write(rss_xml_data)
        log.info(f"Wrote {self.DIGEST_RSS_PATH}")

    def build(self):
        news_article_content, used_articles = (
            self.__get_news_article_content__()
        )
        system_prompt = self.__get_system_prompt__()
        ut = Time.now().ut
        ts = TimeFormat.TIME_ID.format(Time(ut))
        digest_article_list = self.__get_digest_article_list__(
            system_prompt, news_article_content, ts
        )

        self.build_readme(
            used_articles, system_prompt, ts, digest_article_list
        )
        self.build_rss(used_articles, digest_article_list, ut, ts)
